{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes):\n",
    "\t\t# initialize the model along with the input shape\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\t\t# first CONV => RELU => BN layer set\n",
    "\t\tmodel.add(Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\",\n",
    "\t\t\tinput_shape=inputShape))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\t# second CONV => RELU => BN layer set\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\t# first (and only) set of FC => RELU layers\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(128))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\t\tmodel.add(Dropout(0.5))\n",
    "\t\t# softmax classifier\n",
    "\t\tmodel.add(Dense(classes))\n",
    "\t\tmodel.add(Activation(\"softmax\"))\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.losses import MSE\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_image_adversary_targeted(model, image, target_label, eps=2 / 255.0):\n",
    "    # Cast the image to float32\n",
    "    image = tf.cast(image, tf.float32)\n",
    "\n",
    "    # Record gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        # Make prediction on the input image and compute the loss with the target label\n",
    "        pred = model(image)\n",
    "        loss = -MSE(target_label, pred)  # Negative MSE to maximize the distance\n",
    "\n",
    "    # Calculate the gradients of the loss with respect to the image, compute the sign of the gradient\n",
    "    gradient = tape.gradient(loss, image)\n",
    "    signedGrad = tf.sign(gradient)\n",
    "\n",
    "    # Construct the image adversary by subtracting the signed gradient\n",
    "    adversary = (image - (signedGrad * eps)).numpy()\n",
    "    return adversary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST dataset...\n",
      "[INFO] compiling model...\n",
      "[INFO] training network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimheejune/miniconda3/envs/aml/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - accuracy: 0.8836 - loss: 0.3897 - val_accuracy: 0.9780 - val_loss: 0.0695\n",
      "Epoch 2/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - accuracy: 0.9754 - loss: 0.0795 - val_accuracy: 0.9722 - val_loss: 0.0807\n",
      "Epoch 3/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - accuracy: 0.9825 - loss: 0.0588 - val_accuracy: 0.9861 - val_loss: 0.0444\n",
      "Epoch 4/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.9853 - loss: 0.0457 - val_accuracy: 0.9859 - val_loss: 0.0432\n",
      "Epoch 5/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - accuracy: 0.9874 - loss: 0.0385 - val_accuracy: 0.9870 - val_loss: 0.0399\n",
      "Epoch 6/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - accuracy: 0.9915 - loss: 0.0292 - val_accuracy: 0.9883 - val_loss: 0.0370\n",
      "Epoch 7/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - accuracy: 0.9915 - loss: 0.0277 - val_accuracy: 0.9882 - val_loss: 0.0360\n",
      "Epoch 8/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - accuracy: 0.9924 - loss: 0.0237 - val_accuracy: 0.9870 - val_loss: 0.0403\n",
      "Epoch 9/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - accuracy: 0.9931 - loss: 0.0206 - val_accuracy: 0.9882 - val_loss: 0.0363\n",
      "Epoch 10/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - accuracy: 0.9938 - loss: 0.0191 - val_accuracy: 0.9851 - val_loss: 0.0503\n",
      "[INFO] loss: 0.0503, acc: 0.9851\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "#from pyimagesearch.simplecnn import SimpleCNN\n",
    "# from pyimagesearch.fgsm import generate_image_adversary\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# load MNIST dataset and scale the pixel values to the range [0, 1]\n",
    "print(\"[INFO] loading MNIST dataset...\")\n",
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "trainX = trainX / 255.0\n",
    "testX = testX / 255.0\n",
    "# add a channel dimension to the images\n",
    "trainX = np.expand_dims(trainX, axis=-1)\n",
    "testX = np.expand_dims(testX, axis=-1)\n",
    "# one-hot encode our labels\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "# initialize our optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(learning_rate=1e-3)\n",
    "model = SimpleCNN.build(width=28, height=28, depth=1, classes=10)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "# train the simple CNN on MNIST\n",
    "print(\"[INFO] training network...\")\n",
    "model.fit(trainX, trainY,\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tbatch_size=64,\n",
    "\tepochs=10,\n",
    "\tverbose=1)\n",
    "# make predictions on the testing set for the model trained on\n",
    "# non-adversarial images\n",
    "(loss, acc) = model.evaluate(x=testX, y=testY, verbose=0)\n",
    "print(\"[INFO] loss: {:.4f}, acc: {:.4f}\".format(loss, acc))\n",
    "\n",
    "for i in np.random.choice(np.arange(0, len(testX)), size=(10,)):\n",
    "    image = testX[i]\n",
    "    label = testY[i]\n",
    "\n",
    "    # Define the target label (here we just pick class '1' for demonstration)\n",
    "    target_label = np.zeros_like(label)\n",
    "    target_label[1] = 1\n",
    "\n",
    "    # Generate a targeted adversarial image\n",
    "    adversary = generate_image_adversary_targeted(model, image.reshape(1, 28, 28, 1), target_label, eps=0.1)\n",
    "    pred = model.predict(adversary)\n",
    "\n",
    "    # scale both the original image and adversary to the range\n",
    "    # [0, 255] and convert them to an unsigned 8-bit integers\n",
    "    adversary = adversary.reshape((28, 28)) * 255\n",
    "    adversary = np.clip(adversary, 0, 255).astype(\"uint8\")\n",
    "    image = image.reshape((28, 28)) * 255\n",
    "    image = image.astype(\"uint8\")\n",
    "    # convert the image and adversarial image from grayscale to three\n",
    "    # channel (so we can draw on them)\n",
    "    image = np.dstack([image] * 3)\n",
    "    adversary = np.dstack([adversary] * 3)\n",
    "    # resize the images so we can better visualize them\n",
    "    image = cv2.resize(image, (96, 96))\n",
    "    adversary = cv2.resize(adversary, (96, 96))\n",
    "\n",
    "    # determine the predicted label for both the original image and\n",
    "    # adversarial image\n",
    "    imagePred = label.argmax()\n",
    "    adversaryPred = pred[0].argmax()\n",
    "    color = (0, 255, 0)\n",
    "    # if the image prediction does not match the adversarial\n",
    "    # prediction then update the color\n",
    "    if imagePred != adversaryPred:\n",
    "        color = (0, 0, 255)\n",
    "    # draw the predictions on the respective output images\n",
    "    cv2.putText(image, str(imagePred), (2, 25),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.95, (0, 255, 0), 2)\n",
    "    cv2.putText(adversary, str(adversaryPred), (2, 25),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.95, color, 2)\n",
    "    # stack the two images horizontally and then show the original\n",
    "    # image and adversarial image\n",
    "    output = np.hstack([image, adversary])\n",
    "    cv2.imshow(\"FGSM Adversarial Images\", output)\n",
    "    cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
