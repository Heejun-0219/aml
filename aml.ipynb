{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes):\n",
    "\t\t# initialize the model along with the input shape\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\t\t# first CONV => RELU => BN layer set\n",
    "\t\tmodel.add(Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\",\n",
    "\t\t\tinput_shape=inputShape))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\t# second CONV => RELU => BN layer set\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\t# first (and only) set of FC => RELU layers\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(128))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\t\tmodel.add(Dropout(0.5))\n",
    "\t\t# softmax classifier\n",
    "\t\tmodel.add(Dense(classes))\n",
    "\t\tmodel.add(Activation(\"softmax\"))\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.losses import MSE\n",
    "import tensorflow as tf\n",
    "def generate_image_adversary(model, image, label, eps=2 / 255.0):\n",
    "\t# cast the image\n",
    "\timage = tf.cast(image, tf.float32)\n",
    "\t# record our gradients\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\t# explicitly indicate that our image should be tacked for\n",
    "\t\t# gradient updates\n",
    "\t\ttape.watch(image)\n",
    "\t\t# use our model to make predictions on the input image and\n",
    "\t\t# then compute the loss\n",
    "\t\tpred = model(image)\n",
    "\t\tloss = MSE(label, pred)\n",
    "  \t# calculate the gradients of loss with respect to the image, then\n",
    "\t# compute the sign of the gradient\n",
    "\tgradient = tape.gradient(loss, image)\n",
    "\tsignedGrad = tf.sign(gradient)\n",
    "\t# construct the image adversary\n",
    "\tadversary = (image + (signedGrad * eps)).numpy()\n",
    "\t# return the image adversary to the calling function\n",
    "\treturn adversary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST dataset...\n",
      "[INFO] compiling model...\n",
      "[INFO] training network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimheejune/miniconda3/envs/aml/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.8922 - loss: 0.3662 - val_accuracy: 0.9806 - val_loss: 0.0591\n",
      "Epoch 2/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - accuracy: 0.9745 - loss: 0.0807 - val_accuracy: 0.9850 - val_loss: 0.0431\n",
      "Epoch 3/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - accuracy: 0.9825 - loss: 0.0576 - val_accuracy: 0.9851 - val_loss: 0.0448\n",
      "Epoch 4/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13ms/step - accuracy: 0.9868 - loss: 0.0421 - val_accuracy: 0.9855 - val_loss: 0.0450\n",
      "Epoch 5/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.9879 - loss: 0.0392 - val_accuracy: 0.9861 - val_loss: 0.0418\n",
      "Epoch 6/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - accuracy: 0.9903 - loss: 0.0315 - val_accuracy: 0.9876 - val_loss: 0.0398\n",
      "Epoch 7/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - accuracy: 0.9912 - loss: 0.0276 - val_accuracy: 0.9892 - val_loss: 0.0346\n",
      "Epoch 8/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.9911 - loss: 0.0262 - val_accuracy: 0.9895 - val_loss: 0.0335\n",
      "Epoch 9/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.9932 - loss: 0.0212 - val_accuracy: 0.9871 - val_loss: 0.0422\n",
      "Epoch 10/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.9937 - loss: 0.0201 - val_accuracy: 0.9890 - val_loss: 0.0361\n",
      "[INFO] loss: 0.0361, acc: 0.9890\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "#from pyimagesearch.simplecnn import SimpleCNN\n",
    "# from pyimagesearch.fgsm import generate_image_adversary\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# load MNIST dataset and scale the pixel values to the range [0, 1]\n",
    "print(\"[INFO] loading MNIST dataset...\")\n",
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "trainX = trainX / 255.0\n",
    "testX = testX / 255.0\n",
    "# add a channel dimension to the images\n",
    "trainX = np.expand_dims(trainX, axis=-1)\n",
    "testX = np.expand_dims(testX, axis=-1)\n",
    "# one-hot encode our labels\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "# initialize our optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(learning_rate=1e-3)\n",
    "model = SimpleCNN.build(width=28, height=28, depth=1, classes=10)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "# train the simple CNN on MNIST\n",
    "print(\"[INFO] training network...\")\n",
    "model.fit(trainX, trainY,\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tbatch_size=64,\n",
    "\tepochs=10,\n",
    "\tverbose=1)\n",
    "# make predictions on the testing set for the model trained on\n",
    "# non-adversarial images\n",
    "(loss, acc) = model.evaluate(x=testX, y=testY, verbose=0)\n",
    "print(\"[INFO] loss: {:.4f}, acc: {:.4f}\".format(loss, acc))\n",
    "\n",
    "# loop over a sample of our testing images\n",
    "for i in np.random.choice(np.arange(0, len(testX)), size=(10,)):\n",
    "\t# grab the current image and label\n",
    "\timage = testX[i]\n",
    "\tlabel = testY[i]\n",
    "\t# generate an image adversary for the current image and make\n",
    "\t# a prediction on the adversary\n",
    "\tadversary = generate_image_adversary(model,\n",
    "\t\timage.reshape(1, 28, 28, 1), label, eps=0.1)\n",
    "\tpred = model.predict(adversary)\n",
    " \n",
    " \t# scale both the original image and adversary to the range\n",
    "\t# [0, 255] and convert them to an unsigned 8-bit integers\n",
    "\tadversary = adversary.reshape((28, 28)) * 255\n",
    "\tadversary = np.clip(adversary, 0, 255).astype(\"uint8\")\n",
    "\timage = image.reshape((28, 28)) * 255\n",
    "\timage = image.astype(\"uint8\")\n",
    "\t# convert the image and adversarial image from grayscale to three\n",
    "\t# channel (so we can draw on them)\n",
    "\timage = np.dstack([image] * 3)\n",
    "\tadversary = np.dstack([adversary] * 3)\n",
    "\t# resize the images so we can better visualize them\n",
    "\timage = cv2.resize(image, (96, 96))\n",
    "\tadversary = cv2.resize(adversary, (96, 96))\n",
    "\n",
    "\t# determine the predicted label for both the original image and\n",
    "\t# adversarial image\n",
    "\timagePred = label.argmax()\n",
    "\tadversaryPred = pred[0].argmax()\n",
    "\tcolor = (0, 255, 0)\n",
    "\t# if the image prediction does not match the adversarial\n",
    "\t# prediction then update the color\n",
    "\tif imagePred != adversaryPred:\n",
    "\t\tcolor = (0, 0, 255)\n",
    "\t# draw the predictions on the respective output images\n",
    "\tcv2.putText(image, str(imagePred), (2, 25),\n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.95, (0, 255, 0), 2)\n",
    "\tcv2.putText(adversary, str(adversaryPred), (2, 25),\n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.95, color, 2)\n",
    "\t# stack the two images horizontally and then show the original\n",
    "\t# image and adversarial image\n",
    "\toutput = np.hstack([image, adversary])\n",
    "\tcv2.imshow(\"FGSM Adversarial Images\", output)\n",
    "\tcv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
